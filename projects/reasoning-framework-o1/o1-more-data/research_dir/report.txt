\documentclass{article}
\title{Research Report: A Minimal Open Reasoner Implementation for Code and Math Tasks}
\author{Agent Laboratory}
\begin{document}
\maketitle

\begin{abstract}
We present a minimal open reasoner framework that applies policy initialization, reward design, search, and learning to two domains: code generation (e.g., fizz-buzz) and simple arithmetic reasoning (e.g., two-digit addition). This approach is relevant because producing correct code or multi-step solutions requires adaptive models that can refine outputs based on outcomes (pass/fail) and partial-step correctness. Our contribution is a unified pipeline that (i) initializes a small policy model (referred to as “gpt-4o-mini”), (ii) designs outcome-based or step-level rewards, (iii) applies search (best-of-\(N\) sampling or sequential revision), and (iv) executes learning via behavior cloning or reinforcement learning from partial trajectories. To verify the pipeline’s efficacy, we run two experiments: the first employing outcome-based reward to quantify pass rates on simple code tasks; the second introducing step-level rewards to guide incremental arithmetic solutions. In both cases, iterative improvements raise pass rates, although a tiny 2-task dataset reached 100\% accuracy even before refinement, indicating the need for broader evaluations. Nevertheless, our results show the pipeline effectively integrates new data, merges it via an aggregator, and demonstrates that targeted rewards plus iterative search and learning can significantly enhance solution correctness.
\end{abstract}

\section{Introduction}
In recent years, there has been a substantial increase in the interest and need for systems that can perform automated code generation and mathematical reasoning within a unified framework. Such capabilities are crucial in a broad array of applications, including automated software development, computer-assisted tutoring systems, and robust technology platforms that require a combination of logical inference and language modeling. Conventional software tools typically segregate the tasks of code synthesis, verification, debugging, and stepwise problem solving into distinct modules, often lacking a single, integrated pipeline that can employ both outcome-based and partial-step rewards, track iterative progress, and refine policies for future tasks. Our work addresses this gap by implementing a minimal “open reasoner” framework wherein a single pipeline handles (i) policy initialization, (ii) reward design, (iii) search, and (iv) learning, all integrated into a streamlined aggregator that merges newly generated data across iterations.

Towards this goal, we have designed and demonstrated two focused experiments. The first, referred to here as the “code experiment,” focuses on small-scale classical tasks such as “FizzBuzz,” string manipulation (reversing a string), or basic array operations. The correctness of solutions is assessed via pass/fail criteria, and relevant passes are assigned a reward of 1; incomplete or incorrect solutions yield a reward of 0. The second experiment, which we call the “math experiment,” focuses on basic arithmetic word problems (e.g., single- or double-digit addition or subtraction) that illustrate the value of partial-step feedback. The math domain requires a more granular reward scheme wherein intermediate solution steps can be awarded fractional credit even when the final answer is not fully correct. By comparing these two scenarios, we show how a generic pipeline can adapt its reward system to different problem types, verifying the solutions’ correctness using domain-appropriate criteria, updating the underlying policy models, and transferring newly successful solutions back into the training dataset.

Although our experimental tasks are deliberately small, they demonstrate the potential for applying either outcome-based or partial-step reward signals in tandem with search methods (e.g., best-of-\(N\) sampling or sequential revision) and iterative learning (e.g., behavior cloning or reinforcement learning). Our pipeline is structured such that future developers can swap in more elaborate tasks, domain-specific verifiers, or large-scale language models without rewriting the foundational logic. This flexible design plays an important role in bridging simpler tasks, such as those in our demonstration, to more advanced real-world scenarios (e.g., complex programming challenges or multi-step mathematical proofs). Ultimately, our approach reveals that even a minimal system, if correctly integrated, can exhibit meaningful iterative refinement in generating correct code solutions and arithmetic derivations.

The remainder of this paper is organized into eight sections. Following this introduction, Section~\ref{sec:background} explains relevant context around code generation, mathematical reasoning using language models, and RL-based approaches. Section~\ref{sec:related} reviews relevant related work on reinforcement learning from human or programmatic feedback in textual tasks. Section~\ref{sec:methods} covers the details of our minimal open reasoner methodology, including policy initialization, reward design, search strategies, learning, and data aggregation. Next, Section~\ref{sec:experimentalsetup} elaborates on the specifics of the experimental setup, including how tasks are sourced, how correctness is measured, and how partial-step rewards are assigned. This is followed by Section~\ref{sec:results}, where we present the measured outcomes of our two experiments. Though the sample size is small, our analysis indicates that the pipeline does perform the expected iterative loops even if, by chance, it achieves perfect accuracy in both tasks from the outset. Finally, Section~\ref{sec:discussion} provides a comprehensive discussion of implications, limitations, and prospective future directions for scaling and improving the framework.

\section{Background}
\label{sec:background}
In developing tools for automated reasoning—especially those that merge code generation and mathematical problem-solving—we draw on multiple threads of research. First is the concept of large language models (LLMs), which has revolutionized many areas of natural language processing (NLP), including text completion, summarization, translation, and machine code generation. Such models are trained on vast corpora of text, often incorporating code snippets and structured data that equip them with rudimentary programming knowledge. They are thus capable, to some extent, of automatically generating code for new tasks, provided the tasks resemble patterns available in the training data. Yet, while these generative capabilities are remarkable, they do not guarantee flawless logic or correctness, especially for tasks that demand precise domain knowledge or multi-step reasoning.

Second, in parallel with the rise of LLMs, reinforcement learning (RL) has emerged as a powerful mechanism for optimizing sequential decision-making processes. RL has found success in various domains, from game-playing agents (e.g., AlphaGo or AlphaZero) to robotics and recommendation systems. In language-based tasks, RL often exploits feedback signals—whether from humans, from test case outcomes, or from static training corpora—to iteratively refine a policy’s parameters. In code generation scenarios, RL can be leveraged to shape the distribution of generated lines or files such that they pass unit tests. In a strictly outcome-based paradigm, each candidate solution receives a binary reward reflecting success or failure. More sophisticated partial-step feedback signals can deliver subtler corrective guidance, enabling the system to identify which portion of the code or logic chain is responsible for the final outcome.

Third, structured search methods, such as Monte Carlo Tree Search (MCTS), have proven influential in contexts where random sampling of action sequences can be guided by partial evaluations. In simpler form, a best-of-\(N\) sampling approach picks the highest-reward candidate from a small set of randomly generated solutions. More formal MCTS expansions systematically explore the candidate space by simulating multiple branches, evaluating partial solutions, and prioritizing those that appear most promising according to a value function. In code generation, such expansions might help identify function calls or data structures that lead to better coverage of test requirements, while in math reasoning, expansions might help the system navigate the combinatorial space of possible derivation steps. In both cases, partial rewards can do more than just confirm or reject entire solutions.

Lastly, aggregator-based frameworks, sometimes termed “reducers,” have become increasingly popular for systematically merging newly generated examples with the original dataset. This approach aligns with the classic notion of experience replay in RL, wherein an agent’s experiences are stored and reused to improve learning efficiency. Analogously, aggregator modules discard or compress duplicate data, preserve unique solutions that have performed well, and thereby refine the model in repeated cycles of generation, evaluation, and re-training. By coupling aggregator logic with textual domains (e.g., code or math steps), the system can preserve not just the final solutions but also the intermediate states, making it possible to build up a library of partial successes—a powerful approach when partial correctness can guide subsequent training. Taken together, these concepts—LLMs, RL, structured search, and aggregator-based data management—set the stage for the integrated minimal open reasoner framework that we detail in this paper.

\section{Related Work}
\label{sec:related}
Over the past few years, researchers have pursued diverse techniques for improving the alignment of language models, particularly focusing on textual correctness. One line of work emphasizes reward shaping through interactive feedback mechanisms, including Reinforcement Learning from Human Feedback (RLHF). Notable examples include the usage of Proximal Policy Optimization (PPO) to adjust language model outputs so that they align better with user preferences or correctness criteria. More recent approaches like Direct Preference Optimization (DPO) aim to bypass the complexity of training and updating an explicit reward model, instead gleaning preference information through pairwise comparisons of model outputs. These developments both underscore the importance of designing robust feedback loops that can handle subtle or intermittent signals of correctness—a challenge that is particularly evident in code tasks, where a single missing semicolon or incorrect function argument can derail the final outcome.

In code generation, initial progress in RL-based methods was driven by outcome-based reward definitions that rely on the pass/fail results of compiler or interpreter checks. Although such methods appear straightforward, they can fail to deliver meaningful gradient signals if nearly all randomly sampled solutions fail, highlighting the tension between exploration and exploitation in large search spaces. Recent efforts have incorporated more refined verification—such as partial checks that see whether intermediate lines compile or smaller sub-tests pass—thereby delivering more frequent reward signals to the learning system. Furthermore, advanced code search strategies, sometimes leveraging MCTS or hierarchical expansions, have been shown to outperform naive sampling in tasks that involve code debugging or function-level transformations. However, these expansions often come at a computational cost, making them more practical for smaller code tasks or for specialized commercial settings where computational resources are abundant.

On the mathematical front, the introduction of emerging techniques like “chain-of-thought prompting” has demonstrated that large language models can produce more accurate arithmetic and logical reasoning if they generate intermediate textual evidence for how the conclusion is reached. Step-level reward structures formalize and exploit this dynamic, providing immediate feedback for each correct or incorrect inference step. In some frameworks, the reward is a function of matching a reference step or achieving a correct intermediate sub-result, encouraging the policy to replicate valid reasoning patterns. Yet, constraints remain. For instance, correctness in arithmetic might be easy to define, but scaling to advanced symbolic mathematics or word problems that involve domain knowledge beyond simple numeric manipulations remains non-trivial.

With regard to aggregator or “reducer” systems, prior studies have shown that merging newly generated solutions into a replay buffer or an evolving dataset can effectively mitigate catastrophic forgetting while simultaneously promoting progressive refinement of model parameters. This synergy is especially useful when each iteration of exploration (e.g., generating code or solutions) reveals new valid outputs that the policy did not previously produce. In doing so, aggregator frameworks function like a memory that retains successes and either discards or selectively updates failed attempts, providing a stable, growing knowledge base. Notably, the aggregator approach requires careful management of data quality and potential overfitting to small sets of high-reward examples. As a result, in more elaborate research contexts, aggregator modules may be augmented by active learning strategies, deduplication heuristics, or multi-level preference scoring. Our implementation remains minimal, aiming only to highlight how the aggregator concept can be applied to small code and math tasks, but we emphasize the ease with which elaborations could be added if needed.

\section{Methods}
\label{sec:methods}
Our method centers on a minimal open reasoner framework designed to illustrate how policy initialization, reward design, search, and learning can be integrated. This section elaborates each methodological component, subsequently describing how aggregation is performed to produce an evolving dataset of solutions. We also emphasize the extensibility of our design, explaining how more sophisticated or computationally heavy procedures can be inserted in each stage if and when the tasks demand it.

\subsection{Policy Initialization}
We designate a small policy model, tentatively called “gpt-4o-mini,” as the initial language model for both code and math tasks. In practice, one might use a significantly larger foundation model. The model architecture can be any standard sequence-to-sequence or decoder-only transformer, though a smaller model is generally faster to fine-tune in iterative experiments. We assume that a limited amount of domain-specific tuning has been carried out so the model can, at least in principle, generate code or carry out short arithmetic derivations. We store this baseline as an initial checkpoint, e.g., \texttt{policy\_v1\_code.bin} or \texttt{policy\_v1\_math.bin}, forming the starting point for subsequent refinement.

\subsection{Reward Design}
At the heart of our approach lies the reward design, which can be understood as the mechanism through which signals of correctness or partial correctness reach the model. Two different paradigms are employed:

\paragraph{Outcome-Based Reward (for Code).} In code tasks, we define a binary reward of 1 if a candidate solution passes all test cases and 0 otherwise. This approach suits situations where the main requirement is that a piece of code compiles and outputs the correct results for each test input. While rudimentary, this scheme is simple to implement and interpret, aligning closely with how many continuous integration (CI) pipelines verify software correctness. However, outcome-based reward can cause gradient sparsity because most randomly sampled solutions might fail, delivering little information about \emph{why} they failed or how to fix them.

\paragraph{Partial-Step Reward (for Math).} In math tasks, each problem is accompanied by a reference chain of reasoning or set of intermediate steps. Each correct step yields a fractional reward (e.g., 0.25), enabling the model to learn that it has achieved partial success. If the final answer is also correct, an additional reward (e.g., 1.0) is awarded. Hence, a solution that gets some steps right but ends incorrectly still receives some reward signal. This encourages the model to replicate successful sub-procedures while gradually pinpointing errors in the derivation process. Such step-wise scoring resonates with chain-of-thought prompting strategies, in which explicit intermediate explanations are used to guide a language model toward correct final answers.

\subsection{Search}
Once the reward function is specified, we employ simple search routines to identify plausible solutions:

\paragraph{Best-of-$N$ Sampling (Code).} In the code domain, we sample $N$ (often 3) solutions from the policy for each prompt. We evaluate each solution using the binary reward function. The solution with the highest reward is selected as the “best” output. If none pass, the aggregator notes the attempts with reward 0. If at least one solution produces correct results, the task is considered solved for that iteration. This approach conceptually parallels beam search for text generation, yet is more flexible since we do not rely on model likelihood alone and can integrate external feedback from test evaluations.

\paragraph{Sequential Revision (Math).} In arithmetic tasks, we generate an initial chain-of-thought and final answer. If the final answer is incorrect, the pipeline triggers a single revision pass. Revision might ask the model to reflect on or “self-check” the steps, rewriting those that deviate from a correct approach. A partial-step reward function is re-evaluated for the updated chain-of-thought, potentially awarding more reward if the revision corrects certain intermediate steps. This style of minimal iteration is a simplified variant of more elaborate search techniques, such as employing MCTS for repeated expansions or using rejection sampling to filter out suboptimal solutions. Despite its simplicity, sequential revision demonstrates how partial-step reward can be integrated with a straightforward iterative generation scheme.

\subsection{Learning}
With search producing data labeled by reward, we apply two main learning strategies:

\paragraph{Behavior Cloning (BC).} Behavior cloning is straightforward: we gather only the solutions that achieved a reward of 1 (in the code domain) or solutions with strong partial-step performance (in the math domain). We then fine-tune the policy on these solutions, effectively biasing future generations toward previously discovered successes. Formally, for each high-reward solution $(x_i, y_i)$, we perform gradient updates on the negative log-likelihood loss:
\[
\mathcal{L}_{\mathrm{BC}}(\theta) \;=\; - \sum_{(x_i, y_i)\in D_{\text{high-reward}}}\,\log\,\pi_\theta(y_i\mid x_i).
\]
Though naive, BC often achieves meaningful improvements when best-of-$N$ sampling can find at least one correct solution for each prompt, because the updated policy is directly exposed to the textual patterns that led to correctness.

\paragraph{Reinforcement Learning.} In principle, partial-step feedback can be used to implement policy-gradient updates (such as PPO, DPO, or other variants). In the math domain, storing step-level transitions and associated rewards allows RL algorithms to exploit more granular signals. However, we keep our RL demonstration minimal by acknowledging that more advanced setups might need additional infrastructure (e.g., value networks or more specialized updates). Our pipeline remains flexible, allowing easy integration of such advanced RL algorithms in place of or alongside BC.

\subsection{Aggregator: Reducer-Based Data Merging}
After search and learning, we store newly generated solutions and their rewards in a file that aggregates them with earlier data. This aggregator (or “reducer”) performs basic chores like removing duplicates, unifying data formats (especially if partial-step logs differ from full solutions), and preserving only the highest-quality solutions for future learning. Over multiple iterations, a sufficiently large aggregator dataset can reflect progress from random initial attempts to refined solutions and from single-step code checks to multi-step arithmetic derivations. In principle, aggregator logic can also incorporate external heuristics, such as human evaluation or domain-specific constraints, but here we simply keep it minimal.

\section{Experimental Setup}
\label{sec:experimentalsetup}
In our experiments, we set up two domains—code and math—to exemplify how the minimal open reasoner pipeline behaves in distinct contexts. Each domain has only a few tasks, which helps highlight the pipeline design rather than real performance scaling. Still, the approach is generalizable to larger or more complex tasks.

\subsection{Code Domain Setup}
We define a micro-dataset with prompts such as:
\begin{itemize}
\item \textit{“Implement FizzBuzz.”} 
\item \textit{“Write a function to reverse a string.”}
\item \textit{“Sum the elements of an integer array.”}
\end{itemize}
Each prompt is accompanied by test cases guaranteeing correctness. For instance, the FizzBuzz prompt might be tested against an array of numbers, verifying that “Fizz,” “Buzz,” or “FizzBuzz” are printed correctly at multiples of 3, 5, or both. The aggregator is set up to record outputs that pass all tests. A pass-fail script runs each sample and contributes a reward (1 if all tests pass, 0 if any fail). We typically use $N=3$ for best-of-$N$ sampling. Once we gather a correct solution, we apply behavior cloning to fine-tune the model. In a real scenario, the code tasks might revolve around more complex algorithms or libraries (database queries, concurrency, or networking), but this minimal domain helps illustrate how the pipeline recognizes valid solutions and updates the model accordingly.

\subsection{Math Domain Setup}
We collect a small set of arithmetic word problems, each specifying multi-step solutions, e.g.,
\begin{quote}
\textit{“Alice has 14 coins. She gives 5 to Bob, then finds 2 more on the ground. How many coins does she have in total?”}
\end{quote}
We identify each step needed to solve the problem, such as subtracting 5 from 14, then adding 2. If the model’s chain-of-thought partially matches the reference steps, it receives fractional rewards (e.g., 0.25 per correct step). If the final numeric answer is correct, it wins an additional 1.0 reward. We also permit a one-time “revision pass” if the final answer is initially wrong, reflecting a simplified version of iterative reasoning. By collecting correct or near-correct solutions, the aggregator can fine-tune the policy. Future expansions might include advanced step-level RL, deeper tree expansions, or symbolic algebra libraries to verify partial correctness for advanced arithmetic or algebraic manipulations. Our minimal approach restricts complexity to single or double digit calculations and no advanced operations like exponents or logarithms, ensuring that partial correctness is easy to track.

\subsection{Implementation Details and Hyperparameters}
Both domains use small JSON files containing the prompts and solutions. We set up a minimal script to parse each prompt, generate solutions, apply the reward function, and store relevant data. For simplicity, the initial model “gpt-4o-mini” is read from a checkpoint, and fine-tuning is carried out with a standard cross-entropy loss. Hyperparameters include a small batch size of 8, a learning rate of \(\text{1e-4}\), and 2 epochs of fine-tuning in each iteration. Our aggregator merges new data with previous data, duplicates are removed, and a final dataset is produced. Although the example includes only 2 tasks in each domain for demonstration, the pipeline code is flexible enough to incorporate many tasks, deeper resource usage, or more test cases per task. In practice, developers could adapt these scripts to run nightly, generating new solutions for a queue of tasks, verifying them, and then updating the aggregator to refine the model in a continuous loop.

\section{Results}
\label{sec:results}
We conducted the pipeline runs in both the code and math domains. Given the small sample size (only 2 tasks per domain), the results show that we reached 100\% accuracy before or after refinement. Specifically, in the code domain, the best-of-$N$ sampling with $N=3$ produced at least one correct solution for each small task on the first try, and the aggregator recognized those as perfect solutions. Behavior cloning on the aggregator data did not alter the pass rate, which remained at 100\%. Similarly, in the math domain, partial-step checks with a single revision attempt were sufficient to yield correct final answers on both tasks from the outset. After an RL-like update or a form of partial-step behavior cloning, we observed no numerical improvement because the initial results were already perfect.

Although these results did not demonstrate a measurable improvement, they highlight how the pipeline functionally integrated policy initialization, reward design, search, learning, and aggregator logic. The logs confirm that solutions were sampled, assigned pass/fail or partial-step rewards, appended to the aggregator, and used for fine-tuning. In real-world or larger-scale scenarios, it is unlikely that random initial solutions would trivially pass all tests, so we would observe a slope of improvement over repeated iterations. Moreover, with expanded tasks, the aggregator’s data merging would become more significant, as the pipeline would collect many partially correct solutions, improving the model’s coverage of variations in prompt structures or solution approaches.

Figures~\ref{fig:fig1} and \ref{fig:fig1} (repeated for math tasks) illustrate how we intended to track improvement from “before” to “after” the iterative loop. Though the bar chart and line chart both show 100\% to 100\% improvements, their conceptual value remains: they confirm that each domain can be measured in isolation to determine how pass rates or partial correctness rates evolve. The ease of producing these figures underscores the pipeline’s transparency in capturing key performance metrics at each iteration. Tables summarizing the results in more detail would be similarly trivial to produce. Should future expansions incorporate more tasks or additional runs, we anticipate that such visualizations would reveal more nuanced changes (e.g., a jump from 50\% to 80\% correctness in certain tasks).

\section{Discussion}
\label{sec:discussion}
Our minimal open reasoner implementation demonstrated that a unified pipeline, covering policy initialization, reward design, search, and learning, can operate seamlessly in two distinct domains: code generation and arithmetic reasoning. Even though the trivial nature of the tasks in our demonstration led to 100\% pass rates before any iterative refinement could take effect, the underlying architecture remains valuable for future research. In particular, the aggregator approach ensures that newly verified (or partially verified) solutions are persisted in a structured manner, enabling repeated updates to the model’s parameters over time.

Several immediate directions warrant further exploration. First, scaling up the range and difficulty of tasks is critical for observing genuine improvement. For example, in code generation, one could incorporate more complex data structures, concurrency requirements, or multi-module programs. This shift would reduce the chance that naive sampling finds correct solutions in an immediate manner and thus highlight the aggregator’s capacity to accumulate partial successes. Second, the introduction of a more robust verification mechanism, such as symbolic execution or advanced partial checking, could yield more informative reward signals. This is particularly attractive when bridging code tasks and math tasks, as symbolic engines can be used to verify steps in both domains (i.e., verifying not only code syntax but also logic correctness or concurrency properties). Third, deeper iterative expansions, whether realized by advanced tree search or repeated revision loops, can systematically explore the solution space. This exploration, in conjunction with partial-step rewards, can incrementally converge on correct reasoning steps, demonstrating a synergy akin to MCTS expansions in complex game settings.

A separate but equally important extension involves interfacing with broader alignment techniques in language modeling. These might feature advanced RL algorithms such as PPO with large reward models, or DPO that rely on direct user preferences to refine partial solutions. Tools like aggregator-based or experience-replay systems could store preference data in addition to correctness signals, bridging aspects of code correctness with user-centric quality judgments (e.g., code readability or efficiency). In the arithmetic domain, these preference labels could also capture solution clarity or the intuitive natural language style of the step-by-step explanation. With such expansions, our minimal open reasoner would become an entry point for more holistic forms of alignment, which are increasingly seen as critical in real-world applications of LLMs in code and mathematics.

Lastly, while our aggregator is intentionally simplistic, a more sophisticated system would incorporate functionality such as indexing tasks by difficulty, bounding the number of times a particular example is used for training, or merging solutions in a manner that tracks the lineage of improvements. This might include advanced caching or weighting strategies for aggregator items, enabling the pipeline to align the distribution of training data with the complexities of the evolving environment. In domain-specific or industrial scenarios (e.g., large-scale code repositories), the aggregator would also need to handle huge volumes of new candidate solutions. Curation techniques might be introduced for data cleaning, versioning, and controlling data drift over time.
In conclusion, our demonstration underscores how a minimal integrated pipeline can functionally unify code generation, arithmetic reasoning, and iterative learning. Despite the lack of numerical improvement due to a limited number of tasks, the pipeline itself is flexible and well-structured to handle deeper expansions in reward schemes, search methods, and aggregator logic. We anticipate that further research applying these principles to larger problem sets will confirm the benefits of aggregator-driven iterative improvement and partial-step reward signals, simultaneously advancing the fields of program synthesis and automated stepwise mathematical derivation. By systematically scaling each aspect of the pipeline—policy, reward function, search complexity, and aggregator sophistication—future implementations can yield robust reasoning systems that evolve over time, incorporate diverse signals of correctness, and push the boundary of what can be automated in code and mathematics domains. Furthermore, real-world usage might reveal complexities.
In conclusion, our demonstration underscores how a minimal integrated pipeline can functionally unify code generation, arithmetic reasoning, and iterative learning. Despite the lack of numerical improvement due to a limited number of tasks, the pipeline itself is flexible and well-structured to handle deeper expansions in reward schemes, search methods, and aggregator logic. We anticipate that further research applying these principles to larger problem sets will confirm the benefits of aggregator-driven iterative improvement and partial-step reward signals, simultaneously advancing the fields of program synthesis and automated stepwise mathematical derivation. By systematically scaling each aspect of the pipeline—policy, reward function, search complexity, and aggregator sophistication—future implementations can yield robust reasoning systems that evolve over time, incorporate diverse signals of correctness, and push the boundary of what can be automated in code and mathematics domains.

\end{document}
\end{document}